{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "239962eb-a3df-418b-961e-ae2262944606",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "95ce4482-d858-4292-a908-8931903f6b12",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def func_scd2(target,source,key_cols):\n",
    "  \n",
    "    scd_cols = [\"start_date\",\"end_date\",\"is_current\"]\n",
    "\n",
    "    # Identify the columns which are not key or scd columns \n",
    "    hash_columns = list(set(target.columns) - set(key_cols) - set(scd_cols))\n",
    "    print(hash_columns)\n",
    "\n",
    "    # Those columns will help us to identify the insert's and updates\n",
    "    source = source.withColumn('key',F.concat_ws('',*key_cols))\\\n",
    "                   .withColumn(\"hash\",F.sha2(F.concat_ws(\"||\",*hash_columns),256))\n",
    "\n",
    "    target = target.withColumn('key',F.concat_ws('',*key_cols))\\\n",
    "                   .withColumn(\"hash\",F.sha2(F.concat_ws(\"||\",*hash_columns),256))               \n",
    "\n",
    "    # Process to identify the inserts (via key) and updates (via hash)\n",
    "    inserts = source.alias('src').join(target.alias('trg').where('trg.is_current = True'), on='key', how='left')\\\n",
    "                                 .where('trg.key is null')\n",
    "\n",
    "    updates = source.alias('src').join(target.alias('trg').where('trg.is_current = True'), on = 'key', how='inner')\\\n",
    "                                .where('src.hash <> trg.hash')\n",
    "\n",
    "\n",
    "    # Based on this column \"mergekey\" we will perfrom the MERGE operation\n",
    "    inserts = inserts.selectExpr(\"NULL as mergekey\",\"src.*\")\n",
    "    updates = updates.selectExpr(\"key as mergekey\",\"src.*\")\n",
    "\n",
    "\n",
    "    # Union the inserts and updates\n",
    "    final_df_before_ingestion = inserts.unionByName(updates)\n",
    "\n",
    "\n",
    "    # We are manufacturing with parametric way all columns with their values\n",
    "    mergeSet = {\n",
    "      \"active\": True,\n",
    "      \"from\": datetime.now().strftime(\"%d-%b-%Y\"),\n",
    "      \"to\": \"null\",\n",
    "      \"key\": \"src.key\",\n",
    "      \"hash\": \"src.hash\"\n",
    "    }\n",
    "\n",
    "    for col in hash_columns:\n",
    "      mergeSet[col] = 'src.'+col\n",
    "    for col in key_cols:\n",
    "      mergeSet[col] = 'src.'+col\n",
    "\n",
    "    # MERGE: target - final_df_before_ingestion\n",
    "    target.alias('trg').merge(\n",
    "        final_df_before_ingestion.alias('src'),\n",
    "        \"trg.key = src.mergekey\"\n",
    "    )\\\n",
    "    .whenMatchedUpdate(\n",
    "        condition = \"trg.is_current = True\",\n",
    "        set = {\n",
    "            \"is_current\": False,\n",
    "            \"to\": datetime.now().strftime(\"%d-%b-%Y\")\n",
    "    }\n",
    "    )\\\n",
    "    .whenNotMatchedInsert(\n",
    "        values = mergeSet\n",
    "  ).execute()\n",
    "\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions_scd2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}